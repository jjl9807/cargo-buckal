import os
import json
import logging

from typing import List
try:
    from dotenv import load_dotenv
    load_dotenv()
except ImportError:
    pass

from google import genai
from google.genai import types
from github import Github, GithubException, Auth
from unidiff import PatchSet
from pydantic import BaseModel

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Pydantic models for structured output
class ReviewComment(BaseModel):
    file_path: str
    line_number: int
    content: str

class CodeReviewResponse(BaseModel):
    comments: List[ReviewComment]
    summary: str

SYSTEM_INSTRUCTION = """
You are an expert code reviewer.
Analyze the provided code changes (diffs) and provide constructive feedback.
Focus on:
1. Potential bugs and logic errors.
2. Security vulnerabilities.
3. Code style and best practices.
4. Performance improvements.

Output the result in JSON format matching the specified schema.
Only comment on the added lines or lines relevant to the changes.
"""

def get_diff(repo, pr_number):
    pull = repo.get_pull(pr_number)
    return pull.get_files(), pull.head.sha

def parse_and_filter_diff(file_list):
    prompt_context = ""
    valid_files = []
    
    for file in file_list:
        if file.status == "removed" or file.filename.endswith(('.md', '.lock')):
            continue
            
        # Construct a complete diff format (required by unidiff) because file.patch often lacks headers
        diff_content = f"--- a/{file.filename}\n+++ b/{file.filename}\n{file.patch}"
        patch = PatchSet(diff_content)
        for patched_file in patch:
            file_context = f"File: {patched_file.path}\n"
            for hunk in patched_file:
                file_context += f"Hunk at line {hunk.source_start}:\n"
                for line in hunk:
                    # Only send added lines to AI for focused review, while preserving context lines
                    prefix = "+" if line.is_added else ("-" if line.is_removed else " ")
                    # Explicitly mark target line numbers in the prompt to prevent hallucinations
                    line_marker = f"[Line {line.target_line_no}]" if not line.is_removed else ""
                    file_context += f"{prefix} {line_marker} {line.value}"
            
            prompt_context += file_context + "\n---\n"
        valid_files.append(file.filename)
        
    return prompt_context

def analyze_with_gemini(diff_context):
    """Call Gemini 2.0 Flash to generate JSON comments"""
    api_key = os.environ.get("GEMINI_API_KEY")
    if not api_key:
        logger.error("GEMINI_API_KEY not found in environment variables")
        return {"comments": [], "summary": "Review failed: Missing API Key"}
        
    client = genai.Client(api_key=api_key)
    
    prompt = f"""
    Analyze the following code changes. 
    Remember strictly to output English comments only.
    
    CODE CHANGES:
    {diff_context}
    """
    
    try:
        response = client.models.generate_content(
            model="gemini-2.0-flash",
            contents=prompt,
            config=types.GenerateContentConfig(
                system_instruction=SYSTEM_INSTRUCTION,
                response_mime_type="application/json",
                response_schema=CodeReviewResponse
            )
        )
        if response.parsed:
             return response.parsed.model_dump()
        return json.loads(response.text)
    except Exception as e:
        logger.error(f"Gemini inference failed: {e}")
        return {"comments": [], "summary": "AI Review failed."}

def post_github_review(repo, pr_number, commit_sha, review_data):
    """Convert AI results into a GitHub Review request"""
    pull = repo.get_pull(pr_number)
    comments_payload = []
    
    for item in review_data.get("comments", []):
        # Verify if the line number is within the change range (GitHub API limitation: can only comment on lines in the diff)
        # Complex line number validation logic is omitted here; strict matching is required in production
        comments_payload.append({
            "path": item["file_path"],
            "line": item["line_number"],
            "side": "RIGHT", # Usually comment on the added code side
            "body": f"**** {item['content']}"
        })
        
    if not comments_payload:
        logger.info("No issues found.")
        return

    try:
        pull.create_review(
            commit=repo.get_commit(commit_sha),
            body=f"{review_data['summary']}\n\n*Generated by Google Gemini 2.0 Flash*",
            event="COMMENT", # Or "REQUEST_CHANGES" if serious issues are found
            comments=comments_payload
        )
        logger.info("Review posted successfully.")
    except GithubException as e:
        logger.error(f"Failed to post review: {e}")

if __name__ == "__main__":
    github_token = os.environ.get("GITHUB_TOKEN")
    if not github_token:
        logger.error("GITHUB_TOKEN not found in environment variables")
        exit(1)
        
    auth = Auth.Token(github_token)
    g = Github(auth=auth)
    
    repo_name = os.environ.get("REPO_NAME")
    if not repo_name:
        logger.error("REPO_NAME not found")
        exit(1)
    repo = g.get_repo(repo_name)
    
    pr_num_str = os.environ.get("PR_NUMBER")
    if not pr_num_str:
        logger.error("PR_NUMBER not found")
        exit(1)
    pr_num = int(pr_num_str)
    
    files, sha = get_diff(repo, pr_num)
    context = parse_and_filter_diff(files)
    
    # Simple token count protection to prevent overly long requests
    if len(context) > 3000000: # Estimated character limit
        logger.warning("Diff too large for processing.")
    else:
        ai_response = analyze_with_gemini(context)
        post_github_review(repo, pr_num, sha, ai_response)
